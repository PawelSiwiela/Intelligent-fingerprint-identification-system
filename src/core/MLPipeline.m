function MLPipeline(allFeatures, validLabels, metadata, preprocessedImages, validImageIndices, logFile)
% MLPIPELINE Kompleksowy pipeline uczenia maszynowego dla klasyfikacji odcisk√≥w palc√≥w
%
% Funkcja realizuje pe≈Çny cykl trenowania i ewaluacji modeli PatternNet i CNN,
% obejmujƒÖc optymalizacjƒô hiperparametr√≥w, redukcjƒô wymiarowo≈õci, podzia≈Ç danych
% oraz szczeg√≥≈ÇowƒÖ analizƒô por√≥wnawczƒÖ wynik√≥w. Automatycznie zarzƒÖdza zapisem
% optymalnych parametr√≥w i generuje kompleksowe wizualizacje.
%
% Parametry wej≈õciowe:
%   allFeatures - macierz cech [n_samples √ó n_features] dla PatternNet
%   validLabels - wektor etykiet klas [1√ón] odpowiadajƒÖcy pr√≥bkom
%   metadata - struktura z nazwami palc√≥w i informacjami o klasach
%   preprocessedImages - cell array przetworzonych obraz√≥w dla CNN (opcjonalny)
%   validImageIndices - indeksy wa≈ºnych obraz√≥w w preprocessedImages (opcjonalny)
%   logFile - uchwyt pliku do logowania (opcjonalny, domy≈õlnie [])
%
% Dane wyj≈õciowe:
%   - Wytrenowane modele zapisane w output/models/
%   - Wizualizacje por√≥wnawcze w output/figures/
%   - Tabela metryk wydajno≈õci w konsoli
%   - Optymalne parametry dla przysz≈Çego u≈ºycia (przy accuracy ‚â•95%)
%
% Obs≈Çugiwane modele:
%   1. PatternNet - klasyfikacja na podstawie ekstraktowanych cech
%   2. CNN - klasyfikacja bezpo≈õrednio na obrazach (je≈õli dostƒôpne)
%
% Strategia optymalizacji:
%   - Automatyczne wykrywanie zapisanych optymalnych parametr√≥w
%   - Random Search dla nowych hiperparametr√≥w (20-50 pr√≥b)
%   - Inteligentny podzia≈Ç danych [9:2:3] pr√≥bek na klasƒô
%   - Finalne trenowanie na train+validation ‚Üí test
%
% Przyk≈Çad u≈ºycia:
%   % Tylko PatternNet:
%   MLPipeline(features, labels, metadata);
%
%   % PatternNet + CNN:
%   MLPipeline(features, labels, metadata, images, imageIndices, logHandle);

% PARAMETR logFile z domy≈õlnƒÖ warto≈õciƒÖ
if nargin < 6
    logFile = [];
end

fprintf('\n');
fprintf('=================================================================\n');
fprintf('                    ML PIPELINE - FINGERPRINT CLASSIFICATION     \n');
fprintf('                         (PatternNet + CNN)                      \n');
fprintf('=================================================================\n');

% LOGOWANIE ROZPOCZƒòCIA PIPELINE
if ~isempty(logFile)
    logInfo('=============================================================', logFile);
    logInfo('           ML PIPELINE - FINGERPRINT CLASSIFICATION          ', logFile);
    logInfo('=============================================================', logFile);
end

% WALIDACJA ARGUMENT√ìW WEJ≈öCIOWYCH
if nargin < 3
    error('MLPipeline requires at least allFeatures, validLabels, and metadata');
end

% SPRAWDZENIE DOSTƒòPNO≈öCI DANYCH DLA CNN
hasCNNData = (nargin >= 5) && ~isempty(preprocessedImages) && ~isempty(validImageIndices);

if hasCNNData
    fprintf('üìä Running with PatternNet (features) + CNN (images)\n');
    models = {'patternnet', 'cnn'};
    if ~isempty(logFile)
        logInfo('Running with PatternNet (features) + CNN (images)', logFile);
    end
else
    fprintf('üìä Running with PatternNet only (no image data for CNN)\n');
    models = {'patternnet'};
    if ~isempty(logFile)
        logInfo('Running with PatternNet only (no image data for CNN)', logFile);
    end
end

try
    %% KROK 1: PODSUMOWANIE OTRZYMANYCH DANYCH
    fprintf('\nüìÇ Received Data Summary:\n');
    fprintf('‚úÖ Features: %d samples with %d features\n', size(allFeatures, 1), size(allFeatures, 2));
    
    % LOGOWANIE STATYSTYK DANYCH
    if ~isempty(logFile)
        logInfo(sprintf('Features: %d samples with %d features', size(allFeatures, 1), size(allFeatures, 2)), logFile);
    end
    
    if hasCNNData
        fprintf('‚úÖ Images: %d preprocessed images for CNN\n', length(preprocessedImages));
        fprintf('‚úÖ Valid image indices: %d\n', length(validImageIndices));
        if ~isempty(logFile)
            logInfo(sprintf('Images: %d preprocessed images for CNN', length(preprocessedImages)), logFile);
        end
    end
    
    % ANALIZA ROZK≈ÅADU KLAS
    uniqueLabels = unique(validLabels);
    if ~isempty(logFile)
        logInfo('CLASS DISTRIBUTION:', logFile);
    end
    for i = 1:length(uniqueLabels)
        label = uniqueLabels(i);
        count = sum(validLabels == label);
        fingerName = metadata.fingerNames{label};
        fprintf('   %s (label %d): %d samples\n', fingerName, label, count);
        if ~isempty(logFile)
            logInfo(sprintf('%s (label %d): %d samples', fingerName, label, count), logFile);
        end
    end
    
    %% KROK 2: REDUKCJA WYMIAROWO≈öCI
    fprintf('\nüî¨ DIMENSIONALITY REDUCTION FOR PATTERNNET\n');
    reductionStartTime = tic;
    
    [allFeatures, reductionInfo] = askForDimensionalityReduction(allFeatures, validLabels, metadata);
    
    reductionTime = toc(reductionStartTime);
    fprintf('‚úÖ Dimensionality reduction completed in %.2f seconds\n', reductionTime);
    
    %% KROK 3: PODZIA≈Å DANYCH
    fprintf('\nüìä Splitting dataset...\n');
    splittingStartTime = tic;
    
    % STRATEGIA PODZIA≈ÅU zoptymalizowana dla ma≈Çych zbior√≥w danych
    SPLIT_COUNTS = [9, 2, 3]; % Train: 9, Val: 2, Test: 3 pr√≥bki na klasƒô
    fprintf('üîß Using optimized split: [%d, %d, %d] per class\n', SPLIT_COUNTS(1), SPLIT_COUNTS(2), SPLIT_COUNTS(3));
    
    if ~isempty(logFile)
        logInfo(sprintf('Dataset split strategy: [%d, %d, %d] per class', SPLIT_COUNTS(1), SPLIT_COUNTS(2), SPLIT_COUNTS(3)), logFile);
    end
    
    % PODZIA≈Å CECH dla PatternNet
    [trainData, valData, testData] = splitDataset(allFeatures, validLabels, metadata, SPLIT_COUNTS);
    
    % PODZIA≈Å OBRAZ√ìW dla CNN (u≈ºywa tych samych indeks√≥w pr√≥bek)
    imagesData = [];
    if hasCNNData
        fprintf('\nüñºÔ∏è  Splitting images for CNN (using same indices)...\n');
        
        [trainImages, valImages, testImages] = splitImagesDataset(...
            preprocessedImages, validImageIndices, validLabels, metadata, SPLIT_COUNTS);
        
        fprintf('\nüîß Preparing images for CNN training...\n');
        targetSize = [128, 128];
        
        % KONWERSJA OBRAZ√ìW do format√≥w CNN (4D arrays)
        X_train_images = prepareImagesForCNN(trainImages.images, targetSize, true);
        Y_train_images = categorical(trainImages.labels);
        
        X_val_images = prepareImagesForCNN(valImages.images, targetSize, true);
        Y_val_images = categorical(valImages.labels);
        
        X_test_images = prepareImagesForCNN(testImages.images, targetSize, true);
        Y_test_images = categorical(testImages.labels);
        
        % STRUKTURA z danymi obraz√≥w dla CNN
        imagesData = struct();
        imagesData.X_train = X_train_images;
        imagesData.Y_train = Y_train_images;
        imagesData.X_val = X_val_images;
        imagesData.Y_val = Y_val_images;
        imagesData.X_test = X_test_images;
        imagesData.Y_test = Y_test_images;
        
        fprintf('‚úÖ Images prepared for CNN: Train:[%s], Val:[%s], Test:[%s]\n', ...
            mat2str(size(X_train_images)), mat2str(size(X_val_images)), mat2str(size(X_test_images)));
    end
    
    splittingTime = toc(splittingStartTime);
    fprintf('‚úÖ Data splitting completed in %.2f seconds\n', splittingTime);
    
    %% KROK 4: WYB√ìR STRATEGII OPTYMALIZACJI HIPERPARAMETR√ìW
    fprintf('\nüéØ HYPERPARAMETER OPTIMIZATION STRATEGY\n');
    fprintf('%s\n', repmat('=', 1, 60));
    
    % SPRAWDZENIE dostƒôpno≈õci zapisanych optymalnych parametr√≥w
    savedModelsDir = 'output/models';
    availableParameters = checkAvailableOptimalParameters(savedModelsDir, models);
    
    if ~isempty(availableParameters)
        fprintf('Found optimal parameters for: %s\n', strjoin(availableParameters, ', '));
        fprintf('\nChoose optimization strategy:\n');
        fprintf('  1. Use saved optimal parameters (FAST - seconds)\n');
        fprintf('  2. Optimize hyperparameters with Random Search (SLOW - minutes)\n');
        
        while true
            strategy = input('Select strategy (1 or 2): ');
            if ismember(strategy, [1, 2])
                break;
            else
                fprintf('Invalid choice. Please enter 1 or 2.\n');
            end
        end
    else
        fprintf('No saved optimal parameters found. Will use Random Search optimization.\n');
        strategy = 2; % Wymu≈õ optymalizacjƒô
    end
    
    fprintf('\nüìã Selected strategy: ');
    switch strategy
        case 1
            fprintf('Use saved optimal parameters\n');
        case 2
            fprintf('Random Search hyperparameter optimization\n');
    end
    
    %% KROK 5: OPTYMALIZACJA HIPERPARAMETR√ìW DLA KA≈ªDEGO MODELU
    optimizationResults = struct();
    
    for modelIdx = 1:length(models)
        modelType = models{modelIdx};
        
        fprintf('\n%s\n', repmat('=', 1, 60));
        fprintf('üöÄ PROCESSING %s\n', upper(modelType));
        fprintf('%s\n', repmat('=', 1, 60));
        
        if ~isempty(logFile)
            logInfo(sprintf('PROCESSING %s', upper(modelType)), logFile);
        end
        
        % SPRAWDZENIE strategii dla aktualnego modelu
        shouldOptimize = true;
        optimalParams = [];
        
        if strategy == 1
            % STRATEGIA 1: Za≈Çaduj zapisane optymalne parametry
            [optimalParams, loadSuccess] = loadOptimalParameters(savedModelsDir, modelType);
            
            if loadSuccess
                fprintf('‚úÖ Loaded optimal parameters for %s\n', upper(modelType));
                shouldOptimize = false;
                
                % OSZACOWANIE wydajno≈õci na podstawie historycznych danych
                estimatedScore = 0.90; % Konserwatywne oszacowanie dla za≈Çadowanych parametr√≥w
                
                optimizationResults.(modelType) = struct();
                optimizationResults.(modelType).bestHyperparams = optimalParams;
                optimizationResults.(modelType).bestScore = estimatedScore;
                optimizationResults.(modelType).allResults = [];
                optimizationResults.(modelType).source = 'loaded_optimal';
                
                fprintf('üéØ Using optimal %s parameters (estimated score: %.1f%%)\n', ...
                    upper(modelType), estimatedScore * 100);
            else
                fprintf('‚ö†Ô∏è  Failed to load optimal parameters for %s.\n', upper(modelType));
                fprintf('    Switching to optimization...\n');
                strategy = 2; % Automatyczny fallback na optymalizacjƒô
            end
        end
        
        if shouldOptimize
            optimizationStartTime = tic;
            
            % STRATEGIA 2: Optymalizacja hiperparametr√≥w od zera
            if strcmp(modelType, 'cnn')
                numTrials = 20;
            else
                numTrials = 50;
            end
            
            fprintf('üîç Optimizing %s hyperparameters (%d trials)...\n', upper(modelType), numTrials);
            
            if strcmp(modelType, 'cnn') && hasCNNData
                [bestHyperparams, bestScore, results] = optimizeHyperparameters(...
                    trainData, valData, modelType, numTrials, imagesData, logFile);
            else
                [bestHyperparams, bestScore, results] = optimizeHyperparameters(...
                    trainData, valData, modelType, numTrials, [], logFile);
            end
            
            optimizationTime = toc(optimizationStartTime);
            
            optimizationResults.(modelType) = struct(...
                'bestHyperparams', bestHyperparams, ...
                'bestScore', bestScore, ...
                'allResults', results, ...
                'source', 'optimized', ...
                'optimizationTime', optimizationTime);
            
            fprintf('\nüéØ Best %s validation accuracy: %.2f%% (optimization: %.1f sec)\n', ...
                upper(modelType), bestScore * 100, optimizationTime);
        end
    end
    
    %% KROK 6: TRENOWANIE FINALNYCH MODELI na po≈ÇƒÖczonych danych train+validation
    fprintf('\n%s\n', repmat('=', 1, 60));
    fprintf('üèÅ TRAINING FINAL MODELS (Train + Validation Data)\n');
    fprintf('%s\n', repmat('=', 1, 60));
    
    if ~isempty(logFile)
        logInfo('TRAINING FINAL MODELS (Train + Validation Data)', logFile);
    end
    
    finalModels = struct();
    
    for modelIdx = 1:length(models)
        modelType = models{modelIdx};
        
        % SPRAWDZENIE czy optymalizacja siƒô powiod≈Ça
        if ~isfield(optimizationResults, modelType) || optimizationResults.(modelType).bestScore == 0
            fprintf('\n‚ö†Ô∏è  Skipping %s - optimization failed\n', upper(modelType));
            if ~isempty(logFile)
                logWarning(sprintf('Skipping %s - optimization failed', upper(modelType)), logFile);
            end
            continue;
        end
        
        bestHyperparams = optimizationResults.(modelType).bestHyperparams;
        
        fprintf('\nüî• Training final %s model...\n', upper(modelType));
        if ~isempty(logFile)
            logInfo(sprintf('Training final %s model', upper(modelType)), logFile);
        end
        
        try
            if strcmp(modelType, 'cnn') && hasCNNData
                % CNN - trenowanie na obrazach (train+val ‚Üí test)
                fprintf('   üìä Using Train+Val images for final training\n');
                fprintf('   üìà Train images: %d, Val images: %d, Test images: %d\n', ...
                    size(imagesData.X_train, 4), size(imagesData.X_val, 4), size(imagesData.X_test, 4));
                
                [finalModel, trainResults] = trainFinalModelCNN(imagesData, bestHyperparams);
                
            elseif strcmp(modelType, 'patternnet')
                % PatternNet - ≈ÇƒÖczenie train+val dla finalnego trenowania
                fprintf('   üìä Combining Train+Val features for final training\n');
                fprintf('   üìà Train samples: %d, Val samples: %d, Test samples: %d\n', ...
                    length(trainData.labels), length(valData.labels), length(testData.labels));
                
                combinedTrainData = struct();
                combinedTrainData.features = [trainData.features; valData.features];
                combinedTrainData.labels = [trainData.labels; valData.labels];
                
                fprintf('   ‚úÖ Combined training set: %d samples (%.1f%% more data)\n', ...
                    length(combinedTrainData.labels), ...
                    (length(valData.labels) / length(trainData.labels)) * 100);
                
                [finalModel, trainResults] = trainFinalModel(combinedTrainData, testData, modelType, bestHyperparams);
            else
                fprintf('‚ö†Ô∏è  Skipping %s - no data or unsupported type\n', upper(modelType));
                continue;
            end
            
            % DODANIE validation accuracy do wynik√≥w (bezpieczne przypisanie)
            if isfield(optimizationResults, modelType) && isfield(optimizationResults.(modelType), 'bestScore')
                trainResults.valAccuracy = optimizationResults.(modelType).bestScore;  % Z fazy optymalizacji
            else
                trainResults.valAccuracy = 0; % Fallback dla nieprawid≈Çowych danych
            end
            
            finalModels.(modelType) = finalModel;
            finalModels.([modelType '_results']) = trainResults;
            
            fprintf('‚úÖ Final %s test accuracy: %.2f%% (trained on %s)\n', ...
                upper(modelType), trainResults.testAccuracy * 100, ...
                strcmp(modelType, 'cnn'), 'train+val images', 'train+val features');
            
            % ZAPIS OPTYMALNYCH PARAMETR√ìW przy wysokiej jako≈õci (‚â•95% TEST accuracy)
            shouldSaveOptimal = (trainResults.testAccuracy >= 0.95) && ... % Tylko test accuracy!
                isfield(optimizationResults, modelType) && ...
                isfield(optimizationResults.(modelType), 'source') && ...
                strcmp(optimizationResults.(modelType).source, 'optimized'); % Nie z wcze≈õniej za≈Çadowanych
            
            if shouldSaveOptimal
                saveOptimalParameters(modelType, bestHyperparams, trainResults, optimizationResults.(modelType));
                fprintf('üéØ EXCELLENT %.1f%% TEST accuracy! Optimal parameters saved for future use!\n', ...
                    trainResults.testAccuracy * 100);
                if ~isempty(logFile)
                    logInfo(sprintf('EXCELLENT %.1f%% TEST accuracy! Optimal parameters saved', trainResults.testAccuracy * 100), logFile);
                end
            elseif trainResults.testAccuracy >= 0.95 && ...
                    isfield(optimizationResults, modelType) && ...
                    isfield(optimizationResults.(modelType), 'source') && ...
                    strcmp(optimizationResults.(modelType).source, 'loaded_optimal')
                fprintf('üéØ EXCELLENT %.1f%% TEST accuracy achieved with loaded parameters (not re-saving)\n', ...
                    trainResults.testAccuracy * 100);
            elseif trainResults.testAccuracy < 0.95
                fprintf('üìä Test accuracy %.1f%% < 95%% - optimal parameters not saved\n', ...
                    trainResults.testAccuracy * 100);
            end
            
        catch ME
            fprintf('‚ö†Ô∏è  Training %s model failed: %s\n', upper(modelType), ME.message);
            if ~isempty(logFile)
                logError(sprintf('Training %s model failed: %s', upper(modelType), ME.message), logFile);
            end
        end
    end
    
    % IDENTYFIKACJA POMY≈öLNIE WYTRENOWANYCH MODELI
    successfulModels = {};
    for modelIdx = 1:length(models)
        modelType = models{modelIdx};
        if isfield(finalModels, modelType)
            successfulModels{end+1} = modelType;
        end
    end
    
    %% KROK 7: POMIAR SZYBKO≈öCI IDENTYFIKACJI
    if ~isempty(successfulModels)
        fprintf('\n‚ö° MEASURING IDENTIFICATION SPEED...\n');
        fprintf('%s\n', repmat('=', 1, 50));
        
        totalOptimizationTime = 0;
        totalTrainingTime = 0;
        
        for i = 1:length(successfulModels)
            modelType = successfulModels{i};
            model = finalModels.(modelType);
            
            % POMIAR SZYBKO≈öCI IDENTYFIKACJI
            if strcmp(modelType, 'cnn') && hasCNNData
                identResults = measureIdentificationSpeed(model, imagesData.X_test, imagesData.Y_test, modelType);
            else
                identResults = measureIdentificationSpeed(model, testData.features, testData.labels, modelType);
            end
            
            % ZAPISZ WYNIKI SZYBKO≈öCI
            finalModels.([modelType '_speed']) = identResults;
            
            % AKUMULUJ CZASY TRENOWANIA
            if isfield(finalModels, [modelType '_results'])
                trainTime = finalModels.([modelType '_results']).trainTime;
                totalTrainingTime = totalTrainingTime + trainTime;
            end
            
            % AKUMULUJ CZASY OPTYMALIZACJI
            if isfield(optimizationResults, modelType) && isfield(optimizationResults.(modelType), 'optimizationTime')
                optTime = optimizationResults.(modelType).optimizationTime;
                totalOptimizationTime = totalOptimizationTime + optTime;
            end
        end
    end
    
    %% KROK 8: GENEROWANIE WIZUALIZACJI I ANALIZ POR√ìWNAWCZYCH
    fprintf('\nüìä Generating visualizations...\n');
    
    if ~isempty(successfulModels)
        % SZCZEG√ì≈ÅOWE WIZUALIZACJE dla ka≈ºdego modelu
        for i = 1:length(successfulModels)
            modelType = successfulModels{i};
            model = finalModels.(modelType);
            results = finalModels.([modelType '_results']);
            
            if strcmp(modelType, 'cnn')
                % Dla CNN u≈ºyj danych obrazowych testowych
                createModelVisualization(model, results, modelType, imagesData);
            else
                % Dla PatternNet u≈ºyj cech testowych
                createModelVisualization(model, results, modelType, testData);
            end
        end
        
        % POR√ìWNANIE MODELI je≈õli dostƒôpne wiƒôcej ni≈º jeden
        if length(successfulModels) > 1
            compareModels(finalModels, optimizationResults, successfulModels);
        end
    else
        fprintf('‚ö†Ô∏è  No successful models to visualize\n');
    end
    
    %% KROK 9: PODSUMOWANIE KO≈ÉCOWE I REKOMENDACJE
    fprintf('\n%s\n', repmat('=', 1, 60));
    fprintf('üìà FINAL RESULTS SUMMARY\n');
    fprintf('%s\n', repmat('=', 1, 60));
    
    if isempty(successfulModels)
        fprintf('\n‚ùå No models trained successfully!\n');
        return;
    end
    
    % PODSUMOWANIE dla ka≈ºdego pomy≈õlnego modelu
    bestAcc = 0;
    bestModel = '';
    
    for i = 1:length(successfulModels)
        modelType = successfulModels{i};
        results = finalModels.([modelType '_results']);
        valAcc = optimizationResults.(modelType).bestScore * 100;
        testAcc = results.testAccuracy * 100;
        
        fprintf('\n%s:\n', upper(modelType));
        fprintf('  Best validation accuracy: %.2f%%\n', valAcc);
        fprintf('  Final test accuracy:      %.2f%%\n', testAcc);
        fprintf('  Training time:            %.1f seconds\n', results.trainTime);
        
        if results.testAccuracy > 0.95
            fprintf('  üèÜ HIGH PERFORMANCE MODEL SAVED!\n');
        end
        
        % ≈öLEDZENIE najlepszego modelu
        if testAcc > bestAcc
            bestAcc = testAcc;
            bestModel = modelType;
        end
    end
    
    % OG≈ÅOSZENIE ZWYCIƒòZCY
    if ~isempty(bestModel)
        fprintf('\nüèÜ BEST MODEL: %s with %.2f%% test accuracy!\n', upper(bestModel), bestAcc);
        
        % OCENA JAKO≈öCI najlepszego modelu
        if bestAcc > 90
            fprintf('üéâ EXCELLENT PERFORMANCE! Model ready for deployment.\n');
        elseif bestAcc > 80
            fprintf('üòä GOOD PERFORMANCE! Model shows promising results.\n');
        elseif bestAcc > 60
            fprintf('ü§î MODERATE PERFORMANCE! Consider more data or tuning.\n');
        else
            fprintf('üòï POOR PERFORMANCE! Needs significant improvement.\n');
        end
    end
    
    fprintf('\n‚úÖ ML Pipeline completed successfully!\n');
    fprintf('Check output/models/ for saved models.\n');
    fprintf('Check output/figures/ for visualizations.\n');
    
    if ~isempty(logFile)
        logInfo('ML Pipeline completed successfully!', logFile);
    end
    
    if ~isempty(successfulModels)
        fprintf('\n‚è±Ô∏è  COMPLETE TIMING ANALYSIS\n');
        fprintf('===========================\n');
        
        % CZASY PREPROCESSINGU (z metadata)
        if isfield(metadata, 'timings')
            fprintf('üìã PREPROCESSING PHASE:\n');
            fprintf('  üì• Data Loading:        %.2f sec\n', metadata.timings.dataLoading);
            fprintf('  üîÑ Image Processing:    %.2f sec\n', metadata.timings.imagePreprocessing);
            fprintf('  üîç Minutiae Extraction: %.2f sec\n', metadata.timings.minutiaeExtraction);
            fprintf('  üîß Normalization:       %.2f sec\n', metadata.timings.normalization);
            fprintf('  üìä Total Preprocessing: %.2f sec (%.1f min)\n\n', ...
                metadata.timings.totalPreprocessing, metadata.timings.totalPreprocessing/60);
        end
        
        % CZASY MACHINE LEARNING
        fprintf('üìã MACHINE LEARNING PHASE:\n');
        fprintf('  üî¨ Dimensionality Reduction: %.2f sec\n', reductionTime);
        fprintf('  üìä Data Splitting:           %.2f sec\n', splittingTime);
        
        for modelIdx = 1:length(successfulModels)
            modelType = successfulModels{modelIdx};
            
            if isfield(optimizationResults, modelType) && isfield(optimizationResults.(modelType), 'optimizationTime')
                optTime = optimizationResults.(modelType).optimizationTime;
                fprintf('  üéØ %s Optimization:     %.1f sec\n', upper(modelType), optTime);
            end
            
            if isfield(finalModels, [modelType '_results'])
                trainTime = finalModels.([modelType '_results']).trainTime;
                fprintf('  üöÄ %s Final Training:   %.1f sec\n', upper(modelType), trainTime);
            end
            
            % WYNIKI SZYBKO≈öCI IDENTYFIKACJI
            if isfield(finalModels, [modelType '_speed'])
                speedResults = finalModels.([modelType '_speed']);
                fprintf('  ‚ö° %s Identification:   %.2f ms/sample (%.0f samples/sec)\n', ...
                    upper(modelType), speedResults.avgTimeMs, speedResults.throughputSamplesPerSec);
            end
        end
        
        fprintf('  üìä Total ML Processing:      %.2f sec (%.1f min)\n\n', ...
            reductionTime + splittingTime + totalOptimizationTime + totalTrainingTime, ...
            (reductionTime + splittingTime + totalOptimizationTime + totalTrainingTime)/60);
        
        % CA≈ÅKOWITY CZAS SESJI
        if isfield(metadata, 'timings')
            totalSessionTime = metadata.timings.totalPreprocessing + reductionTime + splittingTime + totalOptimizationTime + totalTrainingTime;
            fprintf('üèÅ TOTAL SESSION TIME: %.2f seconds (%.1f minutes)\n', totalSessionTime, totalSessionTime/60);
            
            % BREAKDOWN PROCENTOWY
            fprintf('\nüìä TIME BREAKDOWN:\n');
            fprintf('  Preprocessing: %.1f%% (%.1f sec)\n', (metadata.timings.totalPreprocessing/totalSessionTime)*100, metadata.timings.totalPreprocessing);
            fprintf('  Optimization:  %.1f%% (%.1f sec)\n', (totalOptimizationTime/totalSessionTime)*100, totalOptimizationTime);
            fprintf('  Training:      %.1f%% (%.1f sec)\n', (totalTrainingTime/totalSessionTime)*100, totalTrainingTime);
            fprintf('  Other:         %.1f%% (%.1f sec)\n', ((reductionTime + splittingTime)/totalSessionTime)*100, reductionTime + splittingTime);
            
            % TABELA POR√ìWNAWCZA MODELI
            fprintf('\nüìä MODEL PERFORMANCE COMPARISON:\n');
            fprintf('=====================================\n');
            fprintf('%-12s | %-8s | %-8s | %-12s | %-15s\n', 'Model', 'Val Acc', 'Test Acc', 'Train Time', 'Speed (ms/sample)');
            fprintf('%s\n', repmat('-', 1, 70));
            
            for i = 1:length(successfulModels)
                modelType = successfulModels{i};
                results = finalModels.([modelType '_results']);
                valAcc = optimizationResults.(modelType).bestScore * 100;
                testAcc = results.testAccuracy * 100;
                trainTime = results.trainTime;
                
                if isfield(finalModels, [modelType '_speed'])
                    avgSpeed = finalModels.([modelType '_speed']).avgTimeMs;
                    speedStr = sprintf('%.2f', avgSpeed);
                else
                    speedStr = 'N/A';
                end
                
                fprintf('%-12s | %6.2f%% | %6.2f%% | %8.1fs | %15s\n', ...
                    upper(modelType), valAcc, testAcc, trainTime, speedStr);
            end
        end
    end
    
catch ME
    fprintf('\n‚ùå ML Pipeline error: %s\n', ME.message);
    if ~isempty(logFile)
        logError(sprintf('ML Pipeline error: %s', ME.message), logFile);
    end
    rethrow(ME);
end
end

%% FUNKCJE POMOCNICZE DLA TRENOWANIA FINALNYCH MODELI

function [finalModel, results] = trainFinalModel(trainData, testData, modelType, hyperparams)
% TRAINFINALMODEL Trenuje finalny model PatternNet na cechach

results = struct();
tic;

switch lower(modelType)
    case 'patternnet'
        % UTWORZENIE sieci PatternNet z optymalnymi hiperparametrami
        net = createPatternNet(hyperparams);
        
        X_train = trainData.features';
        T_train = full(ind2vec(trainData.labels', 5));
        
        finalModel = train(net, X_train, T_train);
        
        % TESTOWANIE na zbiorze testowym
        X_test = testData.features';
        Y_test = finalModel(X_test);
        [~, predicted] = max(Y_test, [], 1);
        
        results.testAccuracy = sum(predicted(:) == testData.labels(:)) / length(testData.labels);
        results.predictions = predicted(:);
        results.trueLabels = testData.labels;
        
    otherwise
        error('Unknown model type: %s', modelType);
end

results.trainTime = toc;
results.modelType = modelType;
results.hyperparams = hyperparams;
end

function [finalModel, results] = trainFinalModelCNN(imagesData, hyperparams)
% TRAINFINALMODELCNN Trenuje finalny model CNN na po≈ÇƒÖczonych danych train+val ‚Üí test

results = struct();
tic;

% ≈ÅƒÑCZENIE train + val dla finalnego trenowania (lepsza praktyka)
fprintf('üîó Combining train and validation sets for final CNN training...\n');

X_combined = cat(4, imagesData.X_train, imagesData.X_val);
Y_combined = [imagesData.Y_train; imagesData.Y_val];

fprintf('   Combined training set: [%s] (was [%s] + [%s])\n', ...
    mat2str(size(X_combined)), mat2str(size(imagesData.X_train)), mat2str(size(imagesData.X_val)));

% ZBI√ìR TESTOWY pozostaje niezmieniony
X_test = imagesData.X_test;
Y_test = imagesData.Y_test;

fprintf('   Test set: [%s] (unchanged)\n', mat2str(size(X_test)));

% UTWORZENIE CNN z finalnymi hiperparametrami
inputSize = size(X_combined(:,:,:,1));
cnnStruct = createCNN(hyperparams, 5, inputSize);

fprintf('üöÄ Training final CNN with combined data...\n');

% TRENOWANIE na train+val
finalModel = trainNetwork(X_combined, Y_combined, cnnStruct.layers, cnnStruct.options);

% EWALUACJA tylko na zbiorze testowym
fprintf('üéØ Evaluating on test set...\n');
predicted = classify(finalModel, X_test);
results.testAccuracy = sum(predicted == Y_test) / length(Y_test);
results.predictions = double(predicted);
results.trueLabels = double(Y_test);

results.trainTime = toc;
results.modelType = 'cnn';
results.hyperparams = hyperparams;

fprintf('üìä Final CNN trained on %d samples, tested on %d samples\n', ...
    size(X_combined, 4), size(X_test, 4));
end

function saveOptimalParameters(modelType, hyperparams, results, optimizationResults)
% SAVEOPTIMALPARAMETERS Zapisuje optymalne parametry tylko przy ‚â•95% TEST accuracy

% WALIDACJA: Upewnij siƒô ≈ºe test accuracy rzeczywi≈õcie ‚â•95%
if results.testAccuracy < 0.95
    fprintf('‚ö†Ô∏è  Cannot save optimal parameters - TEST accuracy %.1f%% < 95%%\n', results.testAccuracy * 100);
    return;
end

outputDir = 'output/models';
if ~exist(outputDir, 'dir')
    mkdir(outputDir);
end

timestamp = datestr(now, 'yyyy-mm-dd_HH-MM-SS');
modelTypeLower = lower(modelType);

% NAZWY PLIK√ìW oparte na test accuracy z opisowymi tagami
if results.testAccuracy >= 1.0
    qualityTag = 'perfect_100pct_TEST';
    qualityNote = 'Perfect 100% TEST accuracy achieved through optimization';
    filename = sprintf('%s_OPTIMAL_100PCT_TEST_%s.mat', modelTypeLower, timestamp);
elseif results.testAccuracy >= 0.98
    qualityTag = 'excellent_98pct_TEST';
    qualityNote = 'Excellent 98%+ TEST accuracy achieved through optimization';
    filename = sprintf('%s_OPTIMAL_98PCT_TEST_%s.mat', modelTypeLower, timestamp);
else
    qualityTag = 'very_good_95pct_TEST';
    qualityNote = 'Very good 95%+ TEST accuracy achieved through optimization';
    filename = sprintf('%s_OPTIMAL_95PCT_TEST_%s.mat', modelTypeLower, timestamp);
end

filepath = fullfile(outputDir, filename);

% STRUKTURA z optymalnymi parametrami - podkre≈õlenie test accuracy
optimalData = struct();
optimalData.hyperparams = hyperparams;
optimalData.modelType = modelType;
optimalData.testAccuracy = results.testAccuracy;        % G≈Ç√≥wny wska≈∫nik jako≈õci
optimalData.validationAccuracy = optimizationResults.bestScore; % Wska≈∫nik pomocniczy
optimalData.trainTime = results.trainTime;
optimalData.saveTimestamp = timestamp;
optimalData.source = qualityTag;
optimalData.isHighQuality = true;

% DODANE: Wyra≈∫ne oznaczenie ≈ºe kryterium to test accuracy
optimalData.qualityMetric = 'TEST_ACCURACY';           % Kryterium g≈Ç√≥wne
optimalData.testAccuracyScore = results.testAccuracy;  % Wyra≈∫ne oznaczenie
optimalData.validationAccuracyScore = optimizationResults.bestScore; % Wyra≈∫ne oznaczenie
optimalData.note = qualityNote;

save(filepath, 'optimalData');

fprintf('üéØ HIGH-QUALITY optimal parameters saved: %s\n', filename);
fprintf('   These parameters achieved %.1f%% TEST accuracy (validation: %.1f%%)!\n', ...
    results.testAccuracy * 100, optimizationResults.bestScore * 100);
end

%% FUNKCJE POMOCNICZE DLA ZARZƒÑDZANIA OPTYMALNYMI PARAMETRAMI

function availableModels = checkAvailableOptimalParameters(modelsDir, requestedModels)
% CHECKAVAILABLEOPTIMALPARAMETERS Sprawdza kt√≥re modele majƒÖ zapisane optymalne parametry

availableModels = {};

if ~exist(modelsDir, 'dir')
    return;
end

for i = 1:length(requestedModels)
    modelType = requestedModels{i};
    
    % SZUKANIE tylko plik√≥w z optymalnymi parametrami
    pattern = sprintf('%s_OPTIMAL_*.mat', modelType);
    files = dir(fullfile(modelsDir, pattern));
    
    if ~isempty(files)
        availableModels{end+1} = modelType;
        
        % POKAZANIE najnowszego pliku dla informacji
        [~, newestIdx] = max([files.datenum]);
        newestFile = files(newestIdx);
        
        try
            filePath = fullfile(modelsDir, newestFile.name);
            loadedData = load(filePath);
            
            if isfield(loadedData, 'optimalData')
                accuracy = loadedData.optimalData.testAccuracy * 100;
                fprintf('  %s: %.1f%% TEST accuracy (%s)\n', ...
                    upper(modelType), accuracy, newestFile.name);
            else
                fprintf('  %s: Available (%s)\n', upper(modelType), newestFile.name);
            end
        catch
            fprintf('  %s: Available (%s)\n', upper(modelType), newestFile.name);
        end
    end
end
end

function [optimalParams, success] = loadOptimalParameters(modelsDir, modelType)
% LOADOPTIMALPARAMETERS ≈Åaduje najnowsze optymalne parametry dla modelu

optimalParams = [];
success = false;

if ~exist(modelsDir, 'dir')
    return;
end

% WYSZUKIWANIE plik√≥w z optymalnymi parametrami
pattern = sprintf('%s_OPTIMAL_*.mat', modelType);
files = dir(fullfile(modelsDir, pattern));

if isempty(files)
    return;
end

% SORTOWANIE chronologiczne - najnowsze pliki pierwsze
[~, sortIdx] = sort([files.datenum], 'descend');
selectedFile = files(sortIdx(1));

try
    filePath = fullfile(modelsDir, selectedFile.name);
    loadedData = load(filePath);
    
    if isfield(loadedData, 'optimalData')
        optimalData = loadedData.optimalData;
        optimalParams = optimalData.hyperparams;
        success = true;
        
        % KOMUNIKAT o za≈Çadowanych parametrach
        if isfield(optimalData, 'testAccuracy')
            accuracy = optimalData.testAccuracy * 100;
            
            if accuracy >= 100
                fprintf('üìÇ Loaded PERFECT parameters: %.1f%% TEST accuracy (%s)\n', accuracy, selectedFile.name);
            elseif accuracy >= 98
                fprintf('üìÇ Loaded EXCELLENT parameters: %.1f%% TEST accuracy (%s)\n', accuracy, selectedFile.name);
            elseif accuracy >= 95
                fprintf('üìÇ Loaded VERY GOOD parameters: %.1f%% TEST accuracy (%s)\n', accuracy, selectedFile.name);
            else
                fprintf('üìÇ Loaded parameters: %.1f%% TEST accuracy (%s)\n', accuracy, selectedFile.name);
            end
        else
            fprintf('üìÇ Loaded optimal parameters: %s\n', selectedFile.name);
        end
        
    else
        fprintf('‚ö†Ô∏è  File does not contain optimal parameters: %s\n', selectedFile.name);
    end
    
catch ME
    fprintf('‚ö†Ô∏è  Error loading %s: %s\n', selectedFile.name, ME.message);
end
end

function [reducedFeatures, reductionInfo] = askForDimensionalityReduction(allFeatures, validLabels, metadata)
% ASKFORDIMENSIONALITYREDUCTION Interaktywny wyb√≥r metody redukcji wymiarowo≈õci

fprintf('PatternNet has %d features for %d samples (ratio: %.2f)\n', ...
    size(allFeatures, 2), size(allFeatures, 1), size(allFeatures, 1)/size(allFeatures, 2));
fprintf('Choose dimensionality reduction method:\n');
fprintf('  1. MDA - Multiple Discriminant Analysis (SUPERVISED - most stable)\n');
fprintf('  2. PCA - Principal Component Analysis (UNSUPERVISED)\n');
fprintf('  3. No reduction - Use all original features\n');

while true
    reductionChoice = input('Select option (1, 2, or 3): ');
    if ismember(reductionChoice, [1, 2, 3])
        break;
    else
        fprintf('Invalid choice. Please enter 1, 2, or 3.\n');
    end
end

originalFeatures = allFeatures;
reductionInfo = [];

switch reductionChoice
    case 1 % MDA (Multiple Discriminant Analysis)
        fprintf('üîç Applying MDA (Multiple Discriminant Analysis)...\n');
        params = struct('maxComponents', 4);
        [reducedFeatures, reductionInfo] = reduceDimensionality(allFeatures, 'mda', params, validLabels);
        
        fprintf('üìä MDA Results: %d -> %d features\n', size(originalFeatures, 2), size(reducedFeatures, 2));
        
        try
            visualizeReduction(originalFeatures, reducedFeatures, reductionInfo, validLabels, metadata, 'output/figures');
            fprintf('‚úÖ MDA visualization saved\n');
        catch
            fprintf('‚ö†Ô∏è  MDA visualization failed\n');
        end
        
    case 2 % PCA (Principal Component Analysis)
        fprintf('üîç Applying PCA (unsupervised)...\n');
        params = struct('varianceThreshold', 0.95, 'maxComponents', 15);
        [reducedFeatures, reductionInfo] = reduceDimensionality(allFeatures, 'pca', params);
        
        fprintf('üìä PCA Results: %d -> %d features\n', size(originalFeatures, 2), size(reducedFeatures, 2));
        
        try
            visualizeReduction(originalFeatures, reducedFeatures, reductionInfo, validLabels, metadata, 'output/figures');
            fprintf('‚úÖ PCA visualization saved\n');
        catch
            fprintf('‚ö†Ô∏è  PCA visualization failed\n');
        end
        
    case 3 % BEZ REDUKCJI WYMIAROWO≈öCI
        fprintf('üìä Using all %d original features\n', size(allFeatures, 2));
        reducedFeatures = allFeatures;
        
        % MINIMALNA struktura reductionInfo dla sp√≥jno≈õci
        reductionInfo = struct();
        reductionInfo.method = 'none';
        reductionInfo.originalDims = size(allFeatures, 2);
        reductionInfo.reducedDims = size(allFeatures, 2);
end
end